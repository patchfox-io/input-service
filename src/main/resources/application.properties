# docs for the kafka specific properties
# https://gist.github.com/geunho/77f3f9a112ea327457353aa407328771

# docs for the data specific properties 
# https://docs.spring.io/spring-boot/docs/current/reference/html/application-properties.html


#
# GENERAL 
#

# controls access to the endpoint that allows external callers to put something on the mq. Should ONLY be overridden - NEVER set to true here
patchfox.feature.mq-controller=true

# do NOT use this secret in any environment accessible to the public where the mq-controller feature is enabled.
patchfox.feature.mq-controller.secret=razzle_dazzle_root_beer

patchfox.feature.mq-controller.valid-topics=\
 analyze-service_REQUEST,\
 data-service_REQUEST,\
 forecast-service_REQUEST,\
 grype-service_REQUEST,\
 input-service_REQUEST,\
 nvd-service_REQUEST,\
 orchestrate-service_REQUEST,\
 package-index-service_REQUEST,\
 recommend-service_REQUEST

# dictates what domain can be associated with an event. '*' is a wild card. for production deployments 
# this needs to be the subdomain of the deployment. There is a distinct endpoint to create new datasets and associate
# existing datasources with that new dataset. The reason is there needs to be a "default" dataset to which all 
# events go for production deployments and we can't guarantee that will be the case if users are allowed to 
# dynamically create datasets through the input flow.
patchfox.expected.domain=*

# in case we need to change the default port for some reason 
# server.port=1701

# the name of the service. CHANGE THIS WHEN YOU CREATE A NEW SERVICE 
spring.application.name=input-service

# prevent a caller from uploading a massive file 
spring.servlet.multipart.max-file-size=25MB

# prevent a caller from making a massive request 
spring.servlet.multipart.max-request-size=25MB

# ensures contents of data.sql are injected into db 
spring.sql.init.mode=always

#
# KAFKA
# connection details are overriden in production by k8s
#

# prefix for all kafka consumer clients in this group 
spring.kafka.request.client-id-prefix=${spring.application.name}_REQUEST

# prefix for all kafka producer clients in this group 
spring.kafka.response.client-id-prefix=${spring.application.name}_RESPONSE

# the group name is based of the name of the service. this way multiple instances of the same service can all 
# join the same group and be a happy family
spring.kafka.group-name=${spring.application.name}_GROUP

# service producer topic name
spring.kafka.request-topic=${spring.application.name}_REQUEST

# service consumer topic name
spring.kafka.response-topic=${spring.application.name}_RESPONSE

# how to handle situations where an offset needs to be established or no longer exists 
spring.kafka.consumer.auto-offset-reset=earliest

# necessary to point spring to the advertised network location of the kafka broker 
spring.kafka.bootstrap-servers=0.0.0.0:29092

# number of threads to run in the listener containers 
# *!* you need at least as many partitions as you have consumers
spring.kafka.listener.concurrency=10

# tells kafka to treat submitted message keys as strings 
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer 

# tells kafka to treat submitted message values as json objects
spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer 

# tells kafka to compress outbound messages. options are: 'gzip', 'snappy', 'lz4', 'zstd'
spring.kafka.producer.compression.type=gzip

# tells kafka to treat response keys as strings
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer 

# tells spring to treat response values as json objects
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer

# if we don't set this to warn the logs will be sprayed with a lot of stuff we don't care about 
logging.level.org.apache.kafka=warn

#
# DB 
# connection details in prod are overridden in production by k8s 
#

# JDBC URL of the database.
spring.datasource.url=jdbc:postgresql://0.0.0.0:54321/mrs_db

# login username of the database
spring.datasource.username=mr_data

# login password of the database 
spring.datasource.password=omnomdata

# DDL mode. This is actually a shortcut for the "hibernate.hbm2ddl.auto" property.
spring.jpa.hibernate.ddl-auto=update

# because this
# https://stackoverflow.com/q/26591521/2234770
spring.jpa.properties.hibernate.event.merge.entity_copy_observer=allow


